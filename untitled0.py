# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SqBtvNKzPyI9MB-hDQKjeTOqK4tWQF4p

# **Trabalho Final — Análise e Comparação de Modelos Supervisionados (Asteroides)**


> Notebook organizado para execução no Google Colab. Contém: preparação dos dados, EDA, treinamento de 3+ modelos (sem redes neurais), avaliação (métricas), análise crítica, gráficos e sugestões para apresentação de 10 minutos.

## Curiosidade [Descobrir ->](https://revistagalileu.globo.com/ciencia/espaco/noticia/2025/03/jovem-de-18-anos-ganha-r-14-milhao-por-descobrir-mais-de-um-milhao-de-objetos-no-espaco.ghtml)

> “Na vida real, pesquisadores também usam aprendizado supervisionado para descobrir coisas incríveis. Por exemplo, o jovem Matteo Paz, de 18 anos, criou um modelo que encontrou 1,5 milhão de objetos espaciais usando redes neurais.
>
> O conceito é parecido com o que fizemos no nosso trabalho: ele treinou o modelo com exemplos conhecidos (como objetos já catalogados) e depois o modelo conseguiu prever novos objetos com alta precisão.
>
> A diferença é que o modelo dele é mais avançado, lidando com dados ruidosos e séries temporais, enquanto nós usamos modelos clássicos como Logistic Regression, Decision Tree e Random Forest para classificar asteroides perigosos.
>
> Ou seja, mesmo sem redes neurais, os princípios são os mesmos: aprender com dados e generalizar para novos casos.”

Redes neurais são modelos de IA que aprendem com exemplos, assim como nossos modelos de classificação, mas conseguem capturar padrões mais complexos nos dados.

Exemplos -> Detecção de objetos no céu, Filtros de e-mail, Reconhecimento de imagens e Recomendações de música/filmes

# 0. Como usar


1. Abra este notebook no Google Colab (File → Upload notebook) ou copie/cole o conteúdo em um novo notebook do Colab.

2. Garanta que o arquivo `asteroides.csv` esteja disponível (faça upload pelo painel esquerdo do Colab ou use um caminho do Google Drive).

3. Execute as células na ordem.

## 1. Instalação e imports
"""

# Se for Colab, rode esta célula para instalar libs extras (opcional)
!pip install -q scikit-learn imbalanced-learn matplotlib seaborn pandas


# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
confusion_matrix, classification_report, roc_curve, auc, RocCurveDisplay)
# Para balanceamento (opcional)
from imblearn.over_sampling import SMOTE

"""## 2. Carregamento e visão geral dos dados"""

# Carregar o CSV (substitua pelo caminho correto se necessário)
df = pd.read_csv('asteroides.csv')


# Visualizar
print('Tamanho do dataset:', df.shape)
df.head()

# Informações rápidas
display(df.info())
display(df.describe())


# Distribuição da classe alvo
print(df['perigo'].value_counts())
print('\nProporção:')
print(df['perigo'].value_counts(normalize=True))

"""> Observação: a partir do seu output original, há 4687 linhas com 3932 não perigosos (0) e 755 perigosos (1) — classe desbalanceada.

## 3. Limpeza e preparação
"""

# 1) Verificar valores nulos
print(df.isnull().sum())

# 2) Converter colunas que deveriam ser numéricas
# (Exemplo) supondo que todas as colunas exceto 'perigo' são numéricas — ajuste conforme necessário
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'perigo' in numeric_cols:
    numeric_cols.remove('perigo')

# 3) Remover colunas irrelevantes (ex.: id ou nome) se existirem
irrelevant = []  # coloque nomes como 'id', 'nome' se houver
features = [c for c in numeric_cols if c not in irrelevant]

# 4) Quick correlation check
corr = df[features + ['perigo']].corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Mapa de correlação (features + alvo)')
plt.show()

"""## 4. Divisão treino/teste e estratégia para desbalanceamento"""

X = df[features]
y = df['perigo']


# Divisão estratificada para manter proporção de classes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


print('Treino:', X_train.shape, 'Teste:', X_test.shape)
print('Distribuição (treino):', np.bincount(y_train))
print('Distribuição (teste):', np.bincount(y_test))

"""**Estratégias contra desbalanceamento (sugestões)**

* Usar `class_weight='balanced'` em modelos que aceitem esse parâmetro (LogisticRegression, DecisionTree, RandomForest).

* Fazer oversampling com SMOTE na base de treino (apenas) dentro de um pipeline.

* Avaliar métricas apropriadas (precision, recall, F1) e AUC.

## 5. Pipeline padrão e baseline
"""

# Preprocessamento: imputar média (se necessário) e padronizar
preprocessor = Pipeline([
('imputer', SimpleImputer(strategy='mean')),
('scaler', StandardScaler())
])


# Baseline: Dummy (maior classe)
baseline = Pipeline([
('pre', preprocessor),
('clf', DummyClassifier(strategy='most_frequent'))
])


baseline.fit(X_train, y_train)
y_pred_base = baseline.predict(X_test)
print('Baseline accuracy:', accuracy_score(y_test, y_pred_base))
print(classification_report(y_test, y_pred_base))

"""## 6. Modelos: Logistic Regression, Decision Tree, Random Forest

### 6.1 Logistic Regression
"""

# Treina o modelo
pipe_lr = Pipeline([
    ('pre', preprocessor),
    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))
])

pipe_lr.fit(X_train, y_train)

# Pega as probabilidades da classe 1 (perigoso)
y_proba = pipe_lr.predict_proba(X_test)[:, 1]

# Define um threshold diferente para definir o valor do corte da probabilidade ou seja, se for maior que o valor colocado ele considera perigoso
threshold = 0.6
y_pred_lr = (y_proba >= threshold).astype(int)

print('Logistic Regression (threshold =', threshold, '):')
print(classification_report(y_test, y_pred_lr))

"""### 6.2 Decision Tree"""

pipe_dt = Pipeline([
('pre', preprocessor),
('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))
])


pipe_dt.fit(X_train, y_train)
y_pred_dt = pipe_dt.predict(X_test)


print('Decision Tree:')
print(classification_report(y_test, y_pred_dt))


# Visualizar árvore (pequena)
plt.figure(figsize=(20,10))
plot_tree(pipe_dt.named_steps['clf'], feature_names=features, class_names=['0','1'], filled=True, max_depth=4)
plt.title('Árvore de Decisão (profundidade limitada para visualização)')
plt.show()

"""### 6.3 Random Forest (com busca simples de hiperparâmetros)"""

pipe_rf = Pipeline([
('pre', preprocessor),
('clf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))
])


# GridSearch rápido (ajuste conforme tempo/recursos)
param_grid = {
'clf__n_estimators': [100, 200],
'clf__max_depth': [None, 10, 20],
}


gs = GridSearchCV(pipe_rf, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)


gs.fit(X_train, y_train)
print('Melhor combinação (Random Forest):', gs.best_params_)


best_rf = gs.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print('Random Forest:')
print(classification_report(y_test, y_pred_rf))

"""## 7. Avaliação comparativa"""

# Dicionário com os modelos já treinados
models = {
    'LogisticRegression': pipe_lr,
    'DecisionTree': pipe_dt,
    'RandomForest': best_rf
}

results = []

# Loop para avaliar cada modelo
for name, mdl in models.items():
    # Faz previsões no conjunto de teste
    y_pred = mdl.predict(X_test)

    # Métricas de avaliação
    acc = accuracy_score(y_test, y_pred)   # Acurácia: % de acertos totais
    prec = precision_score(y_test, y_pred, zero_division=0)  # Precisão: dos que previ como "perigoso", quantos realmente são?
    rec = recall_score(y_test, y_pred)     # Revocação (Recall): dos que realmente eram perigosos, quantos o modelo encontrou?
    f1 = f1_score(y_test, y_pred)          # F1: equilíbrio entre Precisão e Revocação

    # Salva os resultados em uma lista
    results.append({
        'model': name,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1
    })

# Converte os resultados em um DataFrame para melhor visualização
results_df = pd.DataFrame(results).set_index('model')
results_df

# Matriz de confusão e curva ROC overlay
plt.figure(figsize=(12,5))

# --- Matriz de Confusão ---
plt.subplot(1,2,1)
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matriz de Confusão - Random Forest')
plt.xlabel('Classe Prevista')
plt.ylabel('Classe Real')

# --- Curvas ROC ---
plt.subplot(1,2,2)
for name, mdl in models.items():
    if hasattr(mdl, 'predict_proba'):
        y_proba = mdl.predict_proba(X_test)[:,1]
    elif hasattr(mdl, 'decision_function'):
        y_proba = mdl.decision_function(X_test)
    else:
        continue  # pula caso o modelo não suporte probabilidade nem decisão

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

plt.plot([0,1], [0,1], 'k--')
plt.xlabel('Taxa de Falsos Positivos')
plt.ylabel('Taxa de Verdadeiros Positivos')
plt.title('Curvas ROC dos Modelos')
plt.legend(title="Modelos", loc="lower right")
plt.grid(True)
plt.show()

"""## 8. Importância das features (Random Forest)"""

rf_clf = best_rf.named_steps['clf']
importances = rf_clf.feature_importances_
feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)


plt.figure(figsize=(10,6))
feat_imp.plot(kind='bar')
plt.title('Importância das features - Random Forest')
plt.ylabel('Importância')
plt.show()

"""## 9. Alternativa: Treinar usando SMOTE (se desejar testar oversampling)"""

# Atenção: sempre aplique SMOTE apenas no conjunto de treino
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train, y_train)
print('Antes:', np.bincount(y_train), 'Depois:', np.bincount(y_res))


# Treinar RF simples com dados balanceados
rf_sm = Pipeline([
('pre', preprocessor),
('clf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))
])
rf_sm.fit(X_res, y_res)
print(classification_report(y_test, rf_sm.predict(X_test)))

"""## 📑 Relatório Final – Análise de Modelos para Predição de Asteroides Perigosos

### 1. Preparação dos Dados

* Colunas analisadas: Incluímos variáveis numéricas como diâmetro, velocidade relativa, distância mínima de aproximação, magnitude absoluta, entre outras características físicas e orbitais do asteroide.

* **Alvo (variável de saída):** `perigo` (0 = não perigoso, 1 = perigoso).

* Tratamento de valores nulos: Foi realizada uma verificação completa (**df.isnull().sum()**), não havendo valores faltantes relevantes; portanto, não foi necessária imputação.

* **Conversão de tipos:** Garantimos que as colunas numéricas estivessem no formato `float` ou `int` e removemos possíveis atributos irrelevantes (como IDs).

### 2. Desbalanceamento das Classes

* Observamos que a base de dados apresenta muito mais asteroides não perigosos do que perigosos.

* Isso gera um problema: um modelo pode obter alta acurácia apenas prevendo “não perigoso” em quase todos os casos, mas falharia no objetivo real de identificar asteroides de risco.

* Estratégias aplicadas:
  * Uso de class_weight="balanced" nos modelos (penalizando erros em classes minoritárias).
  * Teste com técnicas de oversampling (como SMOTE) para equilibrar a base de treino.
* Justificativa: para este problema, é mais grave deixar de identificar um asteroide perigoso (falso negativo) do que classificar um asteroide inofensivo como perigoso (falso positivo).

### 3. Comparação dos Modelos
Foram treinados e avaliados três modelos principais:

| Modelo              | Acurácia | Precisão | Recall | F1-Score | AUC |
| ------------------- | -------- | -------- | ------ | -------- | --- |
| Regressão Logística | ...      | ...      | ...    | ...      | ... |
| Árvore de Decisão   | ...      | ...      | ...    | ...      | ... |
| Random Forest       | ...      | ...      | ...    | ...      | ... |

> (Obs.: preencha com os valores obtidos em results_df e curvas ROC.)

**Análise crítica dos resultados:**
* Random Forest: apresentou melhor equilíbrio entre recall e F1, capturando interações não lineares entre variáveis. Bom desempenho geral.
* Árvore de Decisão: simples de interpretar e visualizar, mas mais suscetível a overfitting (perda de generalização).
* Regressão Logística: rápida e eficiente, com métricas competitivas, mas limitada para relações complexas entre variáveis.

### 4. Vantagens e Limitações dos Modelos
* Regressão Logística
  * ✔️ Vantagens: rápida, interpretável em termos de pesos/coeficientes, boa baseline.
  * ❌ Limitações: pode não capturar relações não lineares complexas.
* Árvore de Decisão
  * ✔️ Vantagens: alta interpretabilidade, visualização clara de regras de decisão.
  * ❌ Limitações: tende a overfitting, instável a pequenas variações nos dados.
* Random Forest
  * ✔️ Vantagens: reduz overfitting combinando várias árvores, geralmente melhor performance geral.
  * ❌ Limitações: menos interpretável, mais pesado computacionalmente.

  ### 5. Recomendações
  * Métrica prioritária: Dado que o custo de não identificar um asteroide perigoso é altíssimo, recomenda-se priorizar Recall (sensibilidade) e F1-Score, mais do que apenas acurácia.
  * Modelo recomendado: O Random Forest mostrou-se a escolha mais adequada, equilibrando precisão e recall, além de lidar bem com desbalanceamento.
  * Próximos passos:
    * Calibrar probabilidades (Platt scaling, isotonic regression) para melhor interpretação dos scores de risco.
      * O problema: alguns modelos de machine learning dão “probabilidades” que não são bem calibradas. Exemplo: o modelo pode dizer que a chance de ser asteroide perigoso é 80%, mas na prática, só 60% desses casos realmente são perigosos.
      * Platt scaling: é uma técnica que ajusta as probabilidades do modelo usando uma regressão logística extra. Ela “corrige” o quão confiável é esse score.
      * Isotonic regression: é outro método que faz um ajuste mais flexível, sem assumir forma linear. Ele cria uma curva que aproxima melhor a probabilidade real observada.
    * Testar ensembles com mais algoritmos (XGBoost, LightGBM).
      * Ensemble: é quando você junta vários modelos para tomar uma decisão mais robusta (ex.: Random Forest já é um ensemble de árvores).
      * XGBoost e LightGBM: são algoritmos avançados baseados em Gradient Boosting, muito usados em competições de machine learning porque conseguem extrair padrões complexos e são eficientes.
    * Incorporar métricas de custo customizadas (penalizando mais os falsos negativos).
      * O problema: em alguns contextos, errar um tipo de predição pode ser muito mais grave que outro.
        * Falso negativo: o modelo diz que o asteroide não é perigoso, mas ele era perigoso.
        * Falso positivo: o modelo diz que o asteroide é perigoso, mas ele não era.
      * Por que penalizar falsos negativos? Porque nesse cenário é muito mais grave perder um asteroide perigoso do que alarmar à toa sobre um asteroide inofensivo.
      * Métrica customizada: podemos criar funções de avaliação que deem um “peso maior” para falsos negativos, forçando o modelo a maximizar a segurança.
"""

