# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SqBtvNKzPyI9MB-hDQKjeTOqK4tWQF4p

# **Trabalho Final ‚Äî An√°lise e Compara√ß√£o de Modelos Supervisionados (Asteroides)**


> Notebook organizado para execu√ß√£o no Google Colab. Cont√©m: prepara√ß√£o dos dados, EDA, treinamento de 3+ modelos (sem redes neurais), avalia√ß√£o (m√©tricas), an√°lise cr√≠tica, gr√°ficos e sugest√µes para apresenta√ß√£o de 10 minutos.

## Curiosidade [Descobrir ->](https://revistagalileu.globo.com/ciencia/espaco/noticia/2025/03/jovem-de-18-anos-ganha-r-14-milhao-por-descobrir-mais-de-um-milhao-de-objetos-no-espaco.ghtml)

> ‚ÄúNa vida real, pesquisadores tamb√©m usam aprendizado supervisionado para descobrir coisas incr√≠veis. Por exemplo, o jovem Matteo Paz, de 18 anos, criou um modelo que encontrou 1,5 milh√£o de objetos espaciais usando redes neurais.
>
> O conceito √© parecido com o que fizemos no nosso trabalho: ele treinou o modelo com exemplos conhecidos (como objetos j√° catalogados) e depois o modelo conseguiu prever novos objetos com alta precis√£o.
>
> A diferen√ßa √© que o modelo dele √© mais avan√ßado, lidando com dados ruidosos e s√©ries temporais, enquanto n√≥s usamos modelos cl√°ssicos como Logistic Regression, Decision Tree e Random Forest para classificar asteroides perigosos.
>
> Ou seja, mesmo sem redes neurais, os princ√≠pios s√£o os mesmos: aprender com dados e generalizar para novos casos.‚Äù

Redes neurais s√£o modelos de IA que aprendem com exemplos, assim como nossos modelos de classifica√ß√£o, mas conseguem capturar padr√µes mais complexos nos dados.

Exemplos -> Detec√ß√£o de objetos no c√©u, Filtros de e-mail, Reconhecimento de imagens e Recomenda√ß√µes de m√∫sica/filmes

# 0. Como usar


1. Abra este notebook no Google Colab (File ‚Üí Upload notebook) ou copie/cole o conte√∫do em um novo notebook do Colab.

2. Garanta que o arquivo `asteroides.csv` esteja dispon√≠vel (fa√ßa upload pelo painel esquerdo do Colab ou use um caminho do Google Drive).

3. Execute as c√©lulas na ordem.

## 1. Instala√ß√£o e imports
"""

# Se for Colab, rode esta c√©lula para instalar libs extras (opcional)
!pip install -q scikit-learn imbalanced-learn matplotlib seaborn pandas


# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
confusion_matrix, classification_report, roc_curve, auc, RocCurveDisplay)
# Para balanceamento (opcional)
from imblearn.over_sampling import SMOTE

"""## 2. Carregamento e vis√£o geral dos dados"""

# Carregar o CSV (substitua pelo caminho correto se necess√°rio)
df = pd.read_csv('asteroides.csv')


# Visualizar
print('Tamanho do dataset:', df.shape)
df.head()

# Informa√ß√µes r√°pidas
display(df.info())
display(df.describe())


# Distribui√ß√£o da classe alvo
print(df['perigo'].value_counts())
print('\nPropor√ß√£o:')
print(df['perigo'].value_counts(normalize=True))

"""> Observa√ß√£o: a partir do seu output original, h√° 4687 linhas com 3932 n√£o perigosos (0) e 755 perigosos (1) ‚Äî classe desbalanceada.

## 3. Limpeza e prepara√ß√£o
"""

# 1) Verificar valores nulos
print(df.isnull().sum())

# 2) Converter colunas que deveriam ser num√©ricas
# (Exemplo) supondo que todas as colunas exceto 'perigo' s√£o num√©ricas ‚Äî ajuste conforme necess√°rio
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'perigo' in numeric_cols:
    numeric_cols.remove('perigo')

# 3) Remover colunas irrelevantes (ex.: id ou nome) se existirem
irrelevant = []  # coloque nomes como 'id', 'nome' se houver
features = [c for c in numeric_cols if c not in irrelevant]

# 4) Quick correlation check
corr = df[features + ['perigo']].corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Mapa de correla√ß√£o (features + alvo)')
plt.show()

"""## 4. Divis√£o treino/teste e estrat√©gia para desbalanceamento"""

X = df[features]
y = df['perigo']


# Divis√£o estratificada para manter propor√ß√£o de classes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


print('Treino:', X_train.shape, 'Teste:', X_test.shape)
print('Distribui√ß√£o (treino):', np.bincount(y_train))
print('Distribui√ß√£o (teste):', np.bincount(y_test))

"""**Estrat√©gias contra desbalanceamento (sugest√µes)**

* Usar `class_weight='balanced'` em modelos que aceitem esse par√¢metro (LogisticRegression, DecisionTree, RandomForest).

* Fazer oversampling com SMOTE na base de treino (apenas) dentro de um pipeline.

* Avaliar m√©tricas apropriadas (precision, recall, F1) e AUC.

## 5. Pipeline padr√£o e baseline
"""

# Preprocessamento: imputar m√©dia (se necess√°rio) e padronizar
preprocessor = Pipeline([
('imputer', SimpleImputer(strategy='mean')),
('scaler', StandardScaler())
])


# Baseline: Dummy (maior classe)
baseline = Pipeline([
('pre', preprocessor),
('clf', DummyClassifier(strategy='most_frequent'))
])


baseline.fit(X_train, y_train)
y_pred_base = baseline.predict(X_test)
print('Baseline accuracy:', accuracy_score(y_test, y_pred_base))
print(classification_report(y_test, y_pred_base))

"""## 6. Modelos: Logistic Regression, Decision Tree, Random Forest

### 6.1 Logistic Regression
"""

# Treina o modelo
pipe_lr = Pipeline([
    ('pre', preprocessor),
    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42))
])

pipe_lr.fit(X_train, y_train)

# Pega as probabilidades da classe 1 (perigoso)
y_proba = pipe_lr.predict_proba(X_test)[:, 1]

# Define um threshold diferente para definir o valor do corte da probabilidade ou seja, se for maior que o valor colocado ele considera perigoso
threshold = 0.6
y_pred_lr = (y_proba >= threshold).astype(int)

print('Logistic Regression (threshold =', threshold, '):')
print(classification_report(y_test, y_pred_lr))

"""### 6.2 Decision Tree"""

pipe_dt = Pipeline([
('pre', preprocessor),
('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))
])


pipe_dt.fit(X_train, y_train)
y_pred_dt = pipe_dt.predict(X_test)


print('Decision Tree:')
print(classification_report(y_test, y_pred_dt))


# Visualizar √°rvore (pequena)
plt.figure(figsize=(20,10))
plot_tree(pipe_dt.named_steps['clf'], feature_names=features, class_names=['0','1'], filled=True, max_depth=4)
plt.title('√Årvore de Decis√£o (profundidade limitada para visualiza√ß√£o)')
plt.show()

"""### 6.3 Random Forest (com busca simples de hiperpar√¢metros)"""

pipe_rf = Pipeline([
('pre', preprocessor),
('clf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))
])


# GridSearch r√°pido (ajuste conforme tempo/recursos)
param_grid = {
'clf__n_estimators': [100, 200],
'clf__max_depth': [None, 10, 20],
}


gs = GridSearchCV(pipe_rf, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)


gs.fit(X_train, y_train)
print('Melhor combina√ß√£o (Random Forest):', gs.best_params_)


best_rf = gs.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print('Random Forest:')
print(classification_report(y_test, y_pred_rf))

"""## 7. Avalia√ß√£o comparativa"""

# Dicion√°rio com os modelos j√° treinados
models = {
    'LogisticRegression': pipe_lr,
    'DecisionTree': pipe_dt,
    'RandomForest': best_rf
}

results = []

# Loop para avaliar cada modelo
for name, mdl in models.items():
    # Faz previs√µes no conjunto de teste
    y_pred = mdl.predict(X_test)

    # M√©tricas de avalia√ß√£o
    acc = accuracy_score(y_test, y_pred)   # Acur√°cia: % de acertos totais
    prec = precision_score(y_test, y_pred, zero_division=0)  # Precis√£o: dos que previ como "perigoso", quantos realmente s√£o?
    rec = recall_score(y_test, y_pred)     # Revoca√ß√£o (Recall): dos que realmente eram perigosos, quantos o modelo encontrou?
    f1 = f1_score(y_test, y_pred)          # F1: equil√≠brio entre Precis√£o e Revoca√ß√£o

    # Salva os resultados em uma lista
    results.append({
        'model': name,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1
    })

# Converte os resultados em um DataFrame para melhor visualiza√ß√£o
results_df = pd.DataFrame(results).set_index('model')
results_df

# Matriz de confus√£o e curva ROC overlay
plt.figure(figsize=(12,5))

# --- Matriz de Confus√£o ---
plt.subplot(1,2,1)
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matriz de Confus√£o - Random Forest')
plt.xlabel('Classe Prevista')
plt.ylabel('Classe Real')

# --- Curvas ROC ---
plt.subplot(1,2,2)
for name, mdl in models.items():
    if hasattr(mdl, 'predict_proba'):
        y_proba = mdl.predict_proba(X_test)[:,1]
    elif hasattr(mdl, 'decision_function'):
        y_proba = mdl.decision_function(X_test)
    else:
        continue  # pula caso o modelo n√£o suporte probabilidade nem decis√£o

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

plt.plot([0,1], [0,1], 'k--')
plt.xlabel('Taxa de Falsos Positivos')
plt.ylabel('Taxa de Verdadeiros Positivos')
plt.title('Curvas ROC dos Modelos')
plt.legend(title="Modelos", loc="lower right")
plt.grid(True)
plt.show()

"""## 8. Import√¢ncia das features (Random Forest)"""

rf_clf = best_rf.named_steps['clf']
importances = rf_clf.feature_importances_
feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)


plt.figure(figsize=(10,6))
feat_imp.plot(kind='bar')
plt.title('Import√¢ncia das features - Random Forest')
plt.ylabel('Import√¢ncia')
plt.show()

"""## 9. Alternativa: Treinar usando SMOTE (se desejar testar oversampling)"""

# Aten√ß√£o: sempre aplique SMOTE apenas no conjunto de treino
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train, y_train)
print('Antes:', np.bincount(y_train), 'Depois:', np.bincount(y_res))


# Treinar RF simples com dados balanceados
rf_sm = Pipeline([
('pre', preprocessor),
('clf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))
])
rf_sm.fit(X_res, y_res)
print(classification_report(y_test, rf_sm.predict(X_test)))

"""## üìë Relat√≥rio Final ‚Äì An√°lise de Modelos para Predi√ß√£o de Asteroides Perigosos

### 1. Prepara√ß√£o dos Dados

* Colunas analisadas: Inclu√≠mos vari√°veis num√©ricas como di√¢metro, velocidade relativa, dist√¢ncia m√≠nima de aproxima√ß√£o, magnitude absoluta, entre outras caracter√≠sticas f√≠sicas e orbitais do asteroide.

* **Alvo (vari√°vel de sa√≠da):** `perigo` (0 = n√£o perigoso, 1 = perigoso).

* Tratamento de valores nulos: Foi realizada uma verifica√ß√£o completa (**df.isnull().sum()**), n√£o havendo valores faltantes relevantes; portanto, n√£o foi necess√°ria imputa√ß√£o.

* **Convers√£o de tipos:** Garantimos que as colunas num√©ricas estivessem no formato `float` ou `int` e removemos poss√≠veis atributos irrelevantes (como IDs).

### 2. Desbalanceamento das Classes

* Observamos que a base de dados apresenta muito mais asteroides n√£o perigosos do que perigosos.

* Isso gera um problema: um modelo pode obter alta acur√°cia apenas prevendo ‚Äún√£o perigoso‚Äù em quase todos os casos, mas falharia no objetivo real de identificar asteroides de risco.

* Estrat√©gias aplicadas:
  * Uso de class_weight="balanced" nos modelos (penalizando erros em classes minorit√°rias).
  * Teste com t√©cnicas de oversampling (como SMOTE) para equilibrar a base de treino.
* Justificativa: para este problema, √© mais grave deixar de identificar um asteroide perigoso (falso negativo) do que classificar um asteroide inofensivo como perigoso (falso positivo).

### 3. Compara√ß√£o dos Modelos
Foram treinados e avaliados tr√™s modelos principais:

| Modelo              | Acur√°cia | Precis√£o | Recall | F1-Score | AUC |
| ------------------- | -------- | -------- | ------ | -------- | --- |
| Regress√£o Log√≠stica | ...      | ...      | ...    | ...      | ... |
| √Årvore de Decis√£o   | ...      | ...      | ...    | ...      | ... |
| Random Forest       | ...      | ...      | ...    | ...      | ... |

> (Obs.: preencha com os valores obtidos em results_df e curvas ROC.)

**An√°lise cr√≠tica dos resultados:**
* Random Forest: apresentou melhor equil√≠brio entre recall e F1, capturando intera√ß√µes n√£o lineares entre vari√°veis. Bom desempenho geral.
* √Årvore de Decis√£o: simples de interpretar e visualizar, mas mais suscet√≠vel a overfitting (perda de generaliza√ß√£o).
* Regress√£o Log√≠stica: r√°pida e eficiente, com m√©tricas competitivas, mas limitada para rela√ß√µes complexas entre vari√°veis.

### 4. Vantagens e Limita√ß√µes dos Modelos
* Regress√£o Log√≠stica
  * ‚úîÔ∏è Vantagens: r√°pida, interpret√°vel em termos de pesos/coeficientes, boa baseline.
  * ‚ùå Limita√ß√µes: pode n√£o capturar rela√ß√µes n√£o lineares complexas.
* √Årvore de Decis√£o
  * ‚úîÔ∏è Vantagens: alta interpretabilidade, visualiza√ß√£o clara de regras de decis√£o.
  * ‚ùå Limita√ß√µes: tende a overfitting, inst√°vel a pequenas varia√ß√µes nos dados.
* Random Forest
  * ‚úîÔ∏è Vantagens: reduz overfitting combinando v√°rias √°rvores, geralmente melhor performance geral.
  * ‚ùå Limita√ß√µes: menos interpret√°vel, mais pesado computacionalmente.

  ### 5. Recomenda√ß√µes
  * M√©trica priorit√°ria: Dado que o custo de n√£o identificar um asteroide perigoso √© alt√≠ssimo, recomenda-se priorizar Recall (sensibilidade) e F1-Score, mais do que apenas acur√°cia.
  * Modelo recomendado: O Random Forest mostrou-se a escolha mais adequada, equilibrando precis√£o e recall, al√©m de lidar bem com desbalanceamento.
  * Pr√≥ximos passos:
    * Calibrar probabilidades (Platt scaling, isotonic regression) para melhor interpreta√ß√£o dos scores de risco.
      * O problema: alguns modelos de machine learning d√£o ‚Äúprobabilidades‚Äù que n√£o s√£o bem calibradas. Exemplo: o modelo pode dizer que a chance de ser asteroide perigoso √© 80%, mas na pr√°tica, s√≥ 60% desses casos realmente s√£o perigosos.
      * Platt scaling: √© uma t√©cnica que ajusta as probabilidades do modelo usando uma regress√£o log√≠stica extra. Ela ‚Äúcorrige‚Äù o qu√£o confi√°vel √© esse score.
      * Isotonic regression: √© outro m√©todo que faz um ajuste mais flex√≠vel, sem assumir forma linear. Ele cria uma curva que aproxima melhor a probabilidade real observada.
    * Testar ensembles com mais algoritmos (XGBoost, LightGBM).
      * Ensemble: √© quando voc√™ junta v√°rios modelos para tomar uma decis√£o mais robusta (ex.: Random Forest j√° √© um ensemble de √°rvores).
      * XGBoost e LightGBM: s√£o algoritmos avan√ßados baseados em Gradient Boosting, muito usados em competi√ß√µes de machine learning porque conseguem extrair padr√µes complexos e s√£o eficientes.
    * Incorporar m√©tricas de custo customizadas (penalizando mais os falsos negativos).
      * O problema: em alguns contextos, errar um tipo de predi√ß√£o pode ser muito mais grave que outro.
        * Falso negativo: o modelo diz que o asteroide n√£o √© perigoso, mas ele era perigoso.
        * Falso positivo: o modelo diz que o asteroide √© perigoso, mas ele n√£o era.
      * Por que penalizar falsos negativos? Porque nesse cen√°rio √© muito mais grave perder um asteroide perigoso do que alarmar √† toa sobre um asteroide inofensivo.
      * M√©trica customizada: podemos criar fun√ß√µes de avalia√ß√£o que deem um ‚Äúpeso maior‚Äù para falsos negativos, for√ßando o modelo a maximizar a seguran√ßa.
"""

